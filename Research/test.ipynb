{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b61bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"thedevastator/python-code-instruction-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d6310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Load CSV\n",
    "dataset = load_dataset(\"csv\", data_files=\"your_file.csv\")[\"train\"]  # single file is loaded as 'train' split\n",
    "\n",
    "# 2. Select only 2 columns\n",
    "dataset = dataset.remove_columns([col for col in dataset.column_names if col not in [\"text\", \"author\"]])\n",
    "\n",
    "# 3. Apply transformation -> make a single column\n",
    "def transform(example):\n",
    "    # Example transformation: concatenate text and author\n",
    "    return {\"input\": example[\"text\"] + \" - \" + example[\"author\"]}\n",
    "\n",
    "dataset = dataset.map(transform)\n",
    "\n",
    "# Drop the original columns\n",
    "dataset = dataset.remove_columns([\"text\", \"author\"])\n",
    "\n",
    "# 4. Split into train/val/test\n",
    "splits = dataset.train_test_split(test_size=0.2, seed=42)  # 80/20\n",
    "test_valid = splits[\"test\"].train_test_split(test_size=0.5, seed=42)  # split 20% into 10/10\n",
    "final_dataset = {\n",
    "    \"train\": splits[\"train\"],\n",
    "    \"validation\": test_valid[\"train\"],\n",
    "    \"test\": test_valid[\"test\"]\n",
    "}\n",
    "\n",
    "print(final_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae01b89d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc6b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13fdc54902284aca987bbbeb0faec2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2').to(device)\n",
    "\n",
    "# Set the EOS token as the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    inputs =  tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "    inputs['labels'] = inputs['input_ids'].copy()\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c22aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='/home/kamal/Desktop/Prodigy/PRODIGY_GA_01/results',\n",
    "    eval_strategy='epoch',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,  \n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='/home/kamal/Desktop/Prodigy/PRODIGY_GA_01/logs'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82c26331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamal/Desktop/Prodigy/PRODIGY_GA_01/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   9/9180 00:19 < 7:10:15, 0.36 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m trainer = Trainer(\n\u001b[32m      2\u001b[39m     model=model,\n\u001b[32m      3\u001b[39m     args=training_args,\n\u001b[32m      4\u001b[39m     train_dataset=tokenized_datasets[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      5\u001b[39m     eval_dataset=tokenized_datasets[\u001b[33m'\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Prodigy/PRODIGY_GA_01/.venv/lib/python3.13/site-packages/transformers/trainer.py:2238\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2236\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Prodigy/PRODIGY_GA_01/.venv/lib/python3.13/site-packages/transformers/trainer.py:2582\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2575\u001b[39m context = (\n\u001b[32m   2576\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2577\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2579\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2580\u001b[39m )\n\u001b[32m   2581\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2582\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2584\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2585\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2586\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2587\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2588\u001b[39m ):\n\u001b[32m   2589\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2590\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Prodigy/PRODIGY_GA_01/.venv/lib/python3.13/site-packages/transformers/trainer.py:3845\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3842\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3843\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3845\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3847\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Prodigy/PRODIGY_GA_01/.venv/lib/python3.13/site-packages/accelerate/accelerator.py:2734\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2732\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2733\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2734\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Prodigy/PRODIGY_GA_01/.venv/lib/python3.13/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Prodigy/PRODIGY_GA_01/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Prodigy/PRODIGY_GA_01/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbb7d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "asasasas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce0eeb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15e9c14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ea260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('../train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179bba73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6ff939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ea703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ab3133",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d586c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaba7e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e9e9a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a129425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a3231c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8174523",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb77bbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../gemini/gemini_21.txt\", \"r\") as f:\n",
    "    kk = f.readlines()\n",
    "print(kk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc90168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = '/home/kamal/Desktop/Prodigy/PRODIGY_GA_01/gemini/'\n",
    "file_len_dict = {}\n",
    "for i in os.listdir(path):\n",
    "    file_len_dict[i] = len(\"\\n\".join(open(path+i).readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4394c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_len_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea51d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"\".join(kk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92b29a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49515a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"I want to check how many tokens I am giving to GPT-2.\"\n",
    "\n",
    "# Encode into tokens\n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Number of tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9419a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7729dbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless=new\")\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.get(r\"https://www.flipkart.com/mens-tshirts/pr?sid=clo,ash,ank,edy&fm=neo%2Fmerchandising&iid=M_624b7d2f-334d-4126-866e-04f303731af5_1_X1NCR146KC29_MC.91BAWC5K7TBY&otracker=hp_rich_navigation_2_1.navigationCard.RICH_NAVIGATION_Fashion~Men%27s%2BTop%2BWear~Men%27s%2BT-Shirts_91BAWC5K7TBY&otracker1=hp_rich_navigation_PINNED_neo%2Fmerchandising_NA_NAV_EXPANDABLE_navigationCard_cc_2_L2_view-all&cid=91BAWC5K7TBY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69632e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(driver.page_source,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f327ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import HTMLSemanticPreservingSplitter\n",
    "splitter = HTMLSemanticPreservingSplitter([(\"h1\", \"Header 1\"), (\"h2\", \"Header 2\")],\n",
    "                                          max_chunk_size=500,chunk_overlap=50,\n",
    "                                          elements_to_preserve=[\"table\", \"ul\"]\n",
    "                                          )\n",
    "docs = splitter.split_text(str(soup.body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8414496",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce9057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert in web data extraction.\n",
    "You will be provided with a website URL. Your task is to extract only the requested information from the website.\n",
    "\n",
    "**Guidelines:**\n",
    "\n",
    "1. Extract only the information that actually exists on the website.\n",
    "\n",
    "* If a piece of information is missing, do not output that, only output the existing information.\n",
    "2. For items such as **privacy policy, return policy, terms & conditions, etc.** that appear as links, return only the URL in this format:\n",
    "\n",
    "```json\n",
    "{{ \"the_key\": {{ \"url\": \"link\" }} }}\n",
    "```\n",
    "3. Do not generate, guess, or add any information that is not present.\n",
    "4. Do not include any explanations, reasoning, or text outside of the required JSON structure. Only output the JSON object.\n",
    "5. Ensure the JSON is valid, properly formatted, and strictly follows the schema below.\n",
    "\n",
    "**Output Format (strict JSON):**\n",
    "\n",
    "{{\"Hero Products\":\n",
    "    {{\"product-name\":\"Name of the product\",\n",
    "    \"product-url\":\"url of the product\",\n",
    "    \"image-url\":\"url of the image\",\n",
    "    \"price\":\"price of the product\"}}\n",
    "\"Privacy Policy\":\n",
    "    {{\"1\":\"first policy\",\n",
    "    \"2\":\"the second policy\",\n",
    "    ...}}\n",
    "\"Return, Refund Policies\":\n",
    "    {{\"1\":\"the policy\",\n",
    "    \"2\":\"the second policy\",\n",
    "    ...}}\n",
    "\"Social Handles\":\n",
    "    {{\"facebook\":\"url\",\n",
    "    \"instagram\":\"url\",\n",
    "    ...}}\n",
    "\"Contact Details\":\n",
    "    {{\"phone number\":\"the number\",\n",
    "    \"email\":\"email_id\",\n",
    "    ...}}\n",
    "\"About the Brand\":\"about the brand details\",\n",
    "\"Important links\":\n",
    "    {{\"Order tracking\":\"the url\",\n",
    "    \"Contact Us\":\"contact us url\",\n",
    "    \"Blogs\":[\"url1\",\"url2\"]\n",
    "    ...}}\n",
    "}}\n",
    "Here is a chunk of html code of the website:\n",
    "{html_code}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4fb073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10\n",
    ")\n",
    "hf = HuggingFacePipeline(pipeline=pipe)\n",
    "chain2 = prompt_template | hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58ffa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(['hi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a484ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7384f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.invoke('who are you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563be2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2.invoke({'html_code':docs[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faafd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_reponse = [get_response(i) for i in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26807957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1494012c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e56dbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_list = [len(tokenizer.encode(prompt_template.invoke({\"html_code\": i.page_content}).text)) for i in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a76d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in the_list if i>=1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156aab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.mean(the_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0d19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/home/kamal/.env')\n",
    "import os\n",
    "gemini_api = os.getenv('GOOGLE_API_KEY')\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",api_key=gemini_api)\n",
    "chain = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c9190",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({'html_code':docs[0].page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f17fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e248b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10\n",
    ")\n",
    "llm2 = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ab5d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm2.invoke(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af13bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = prompt_template | llm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6465b7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kkkk=r\"\"\"i will provide you latex code for my resume and you will always provide me the full latex code of optimised resume. do not change the latex resume design, style or blueprint. Highlight matched keyworks with bold letters (\\textbf). Do not give preamble or explanation or any otherthings except the full latex resume, unless i asked you to. Here is my resume ------------------- \\section{SUMMARY} \\begin{onecolentry} IBM-certified Data Scientist with a proven track record in designing and deploying data-driven solutions. Leveraged advanced statistical analysis, machine learning (including LSTM and NLP), and automation techniques to drive a 35\\% increase in user engagement and reduce manual processes by up to 40\\% during internships and open-source projects. Seeking to bring analytical rigor and innovative problem-solving to a forward-thinking organization. \\end{onecolentry} \\section{SKILLS} \\begin{onecolentry} \\begin{highlights} \\item \\textbf{Programming:} Python (NumPy, Pandas, Matplotlib, Seaborn, Plotly, Scikit-Learn, TensorFlow, Keras, PyTorch, Streamlit, OpenCV) \\item \\textbf{Data Analysis:} Statistical Analysis, Data Visualization, Data Preprocessing, Exploratory Data Analysis (EDA) \\item \\textbf{Machine Learning:} Supervised \\& Unsupervised Learning, LSTM Networks, Natural Language Processing (NLP), Classification (Decision Trees, SVM, Random Forests), Clustering (K-means, Hierarchical) \\item \\textbf{Deep Learning:} Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Image Classification, Object Detection \\item \\textbf{Agentic Tools:} LangChain, LangGraph \\item \\textbf{Tools:} PostgrSQL, MySQL, MongoDB, Power BI, Git \\& GitHub, Microsoft Excel, PowerBI, Docker \\item \\textbf{Soft Skills:} Communication, Problem Solving, Critical Thinking, Team Collaboration, Time Management \\end{highlights} \\end{onecolentry} \\section{EXPERIENCE} \\begin{twocolentry}{ \\textit{Aug 2025 - Present} } \\textbf{Generative AI Intern}, Prodigy InfoTech | Remote \\end{twocolentry} \\vspace{0.10 cm} \\begin{onecolentry} \\begin{highlights} \\item Designed and deployed a Markov Chain-based text generator, experimenting with token-level prediction to generate diverse and coherent text outputs, enhancing text automation capabilities. \\item Fine-tuned GPT-2 on custom datasets to retrieve relevant information from web-scraped HTML pages, improving context relevance and knowledge grounding by 30\\%. \\item Applied \\textbf{Pix2Pix GANs} for style transfer between images, successfully transforming visual aesthetics while maintaining structural integrity, showcasing deep learning creativity in real-world applications. \\end{highlights} \\end{onecolentry} \\vspace{0.2 cm} \\begin{twocolentry}{ \\textit{Aug 2024 - Dec 2024} } \\textbf{Open Source Contributor}, Resume Matcher | \\href{https://resume-matcher-c.streamlit.app/}{\\underline{WebSite}} | \\href{https://github.com/KamalMahanna/Resume-Matcher.git}{\\underline{GitHub}} \\end{twocolentry} \\vspace{0.10 cm} \\begin{onecolentry} \\begin{highlights} \\item Automated the creation of missing directories to resolve critical project initialization issues. \\item Fixed Docker-compose initialization failuare by implementing automated directory creation, eliminating deployment failures. \\item Integrated flake8 with git commit hooks to enforce code quality standards across the project. \\end{highlights} \\end{onecolentry} \\vspace{0.2 cm} \\begin{twocolentry}{ \\textit{Jul 2023 - Oct 2023} } \\textbf{Data Science Intern}, Industry Academia Community | \\href{https://iac-chatbot.streamlit.app/}{\\underline{WebSite}} | \\href{https://github.com/KamalMahanna/IAC-Chatbot}{\\underline{GitHub}} \\end{twocolentry} \\vspace{0.10 cm} \\begin{onecolentry} \\begin{highlights} \\item Developed an AI-powered chatbot using Google’s Dialogflow, increasing user engagement by 35\\% among a community of over 5,000 members. \\item Leveraged natural language processing (NLP) techniques to enhance response accuracy by 25\\%. \\item Reduced support response times by 20\\% through efficient design and implementation. \\end{highlights} \\end{onecolentry} \\vspace{0.2 cm} \\begin{twocolentry}{ \\textit{Jul 2023 - Aug 2023} } \\textbf{Artificial Intelligence Intern}, Bharat Intern | \\href{https://stock-price-kkm.streamlit.app/}{\\underline{WebSite}} | \\href{https://github.com/kamalmahanna/stock-price-prediction}{\\underline{GitHub}} \\end{twocolentry} \\vspace{0.10 cm} \\begin{onecolentry} \\begin{highlights} \\item Engineered a stock price prediction model using LSTM networks, achieving RMSE values of 0.00045 (univariate) and 0.00016 (multivariate). \\item Performed exploratory data analysis (EDA) to identify key patterns, enhancing forecasting accuracy by 15\\%. \\item Applied advanced statistical techniques on historical trading data to improve ROI for trading portfolios. \\end{highlights} \\end{onecolentry} \\vspace{0.2 cm} \\begin{twocolentry}{ \\textit{Oct 2022 - Apr 2023} } \\textbf{Data Science Intern}, Ai Variant | \\href{https://kamalmahanna.github.io/Book-Recommendation-System/}{\\underline{Book Recommender}} | \\href{https://github.com/KamalMahanna/Resume-Classification-and-Parser.git}{\\underline{GitHub}} \\end{twocolentry} \\vspace{0.10 cm} \\begin{onecolentry} \\begin{highlights} \\item Developed a Book Recommendation System using content-based and collaborative filtering, resulting in a 10\\% boost in user retention. \\item Cleaned and processed over 5,000 null values by web scrapping, improving recommendation engine performance by 30\\%. \\item Led the development of an Automated Resume Classification System with NLP and supervised learning, cutting manual HR effort by 40\\%. \\item Built robust preprocessing pipelines, enhancing resume parsing and classification accuracy by 30\\%. \\end{highlights} \\end{onecolentry} \\section{PROJECTS} \\begin{onecolentry} \\textbf{Audio Chat - Local AI Text \\& Voice Assistant} | \\href{https://github.com/KamalMahanna/Audio-Chat.git}{\\underline{GitHub}} \\end{onecolentry} \\vspace{0.10 cm} \\begin{onecolentry} \\begin{highlights} \\item Identified the growing need for private, cloud-free AI interactions and designed a local-first chat platform enabling secure text and audio conversations. \\item Aimed to ensure 100\\% on-device AI processing while supporting voice interaction, persistent history, and seamless deployment. \\item Built a full-stack application using FastAPI, MongoDB, and Docker, integrating Ollama + LangChain for local LLMs, Whisper for speech-to-text, and Kokoro-TTS for natural voice output. \\item Delivered a fully private AI assistant with persistent local chat history, instant deployment via Docker Compose, and complete cloud independence, empowering privacy-conscious users with modern AI capabilities. \\end{highlights} \\end{onecolentry} \\vspace{0.2 cm} \\begin{onecolentry} \\textbf{AI Career Assistant Tools} | \\href{https://ai-resume-creator.netlify.app/}{\\underline{Resume Creator}} | \\href{https://ai-career-helper.netlify.app/}{\\underline{Career Helper}} \\end{onecolentry} \\vspace{0.10 cm} \\begin{onecolentry} \\begin{highlights} \\item Built an AI-powered platform using Python, Gemini API, and Prompt Engineering, integrating resume generation, ATS scoring, and interview preparation tools to streamline job search and boost candidate success. \\item Automated the creation of ATS-optimized, customizable resumes, reducing preparation time using Gemini API and enhancing relevance for recruiters. \\item Added features like text summarization and personalized career guidance with NLP techniques, offering actionable insights for professional development. \\item Combined multiple career tools into a cohesive web app using Flask (backend) and React (frontend), improving user experience and functionality. \\end{highlights} \\end{onecolentry} \\vspace{0.2 cm} \\begin{onecolentry} \\textbf{YT-QnA – YouTube Video Question Answering App} | \\href{https://yt-qna.streamlit.app/}{\\underline{WebApp}} | \\href{https://github.com/KamalMahanna/YT-QnA}{\\underline{GitHub}} \\end{onecolentry} \\vspace{0.10 cm} \\begin{onecolentry} \\begin{highlights} \\item Developed an AI-powered Streamlit application that enables users to ask natural language questions about YouTube videos and receive context-aware answers. \\item Implemented transcript extraction using YouTube API and Whisper (via Groq API), followed by semantic chunking and storage in ChromaDB for efficient retrieval. \\item Integrated Gemini LLM to generate precise answers and audio summaries based on retrieved transcript segments, improving information accessibility from lengthy videos. \\item Designed a scalable Docker-based deployment pipeline, enabling persistent vector database storage and seamless setup via containerization. \\end{highlights} \\end{onecolentry} \\section{EDUCATION} \\begin{twocolentry}{ \\textit{Nov 2022} } \\textbf{B.Tech in Electrical Engineering}, Biju Patnaik University of Technology \\end{twocolentry} \\vspace{0.10 cm} \\begin{onecolentry} \\begin{highlights} \\item CGPA: 8.79/10 \\end{highlights} \\end{onecolentry} \\section{CERTIFICATIONS} \\begin{onecolentry} \\begin{highlights} \\item Data Science Certification from ExcelR \\item Machine Learning Certification from \\textbf{IBM} \\item 5-Star Python Coder from \\textbf{HackerRank} \\end{highlights} \\end{onecolentry} \\section{ACHIEVEMENTS} \\begin{onecolentry} \\begin{highlights} \\item Awarded \"Unique and Best Data Scientist\" for independently implementing null value imputation via web scraping, setting a new standard for data quality and efficiency during the Ai Variant internship. \\end{highlights} \\end{onecolentry} ------------------- here is the job description ------------------- Job description Roles and Responsibilities Design, develop, test, and deploy AI-powered solutions using various technologies such as AIMl, Deep Learning, and Machine Learning. Collaborate with cross-functional teams to identify business problems and implement automation solutions using tools like Blue Prism, Uipath, or Automation Anywhere. Develop data analysis skills to extract insights from large datasets and provide recommendations for process improvements. Troubleshoot issues related to deployment automation and ensure seamless integration with existing systems. Desired Candidate Profile 0-1 year of experience in Artificial Intelligence Engineering or a related field. Strong understanding of AI algorithms, machine learning algorithms, and deep learning concepts. Proficiency in programming languages such as Python Role: Automation Developer Industry Type: IT Services & Consulting Department: Engineering - Software & QA Employment Type: Full Time, Permanent Role Category: Software Development\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa31c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(kkkk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb23bf3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8965ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[40].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aad2ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain1.invoke({'html_code':docs[0].page_content})\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df68ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gemini = [chain.invoke({\"html_code\": i.page_content}).content for i in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f73684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gpt2 = [chain1.invoke({\"html_code\": i.page_content}).content for i in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b6b7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdadb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results_gemini.txt','w+') as f:\n",
    "    lines = f.writelines(results_gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4236f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['html_text','json_output'],data=[[i.page_content for i in docs],\n",
    "                                                            ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6572025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re.findall(r\"```json\\s*([\\s\\S]*?)```\",results_gemini[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1880c03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b30c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from test import get_response\n",
    "get_response(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e410237",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in enumerate(results_gemini):\n",
    "    with open(f'gemini/gemini_{i}.txt', 'w') as f:\n",
    "        f.write(j)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f0df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.page_content for i in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7530941b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb62bace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455249c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589b70cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbb1c77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d678840b",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18619f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4100e9a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0479ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Set up headless mode (optional)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless=new\")\n",
    "# service = Service(executable_path = '/home/kamal/Desktop/Prodigy/PRODIGY_GA_01/Research/chromedriver')\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome(options=chrome_options,\n",
    "                        #   service=service\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22294c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.myntra.com/clothing\")\n",
    "source_html = driver.page_source\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8542de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(source_html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562bd4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e56638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd10ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b6037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    # Navigate to a website\n",
    "    driver.get(\"https://www.example.com\")\n",
    "\n",
    "    # Wait for an element to be present\n",
    "    element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"some_element_id\"))\n",
    "    )\n",
    "\n",
    "    # Extract text from the element\n",
    "    data = element.text\n",
    "    print(f\"Extracted data: {data}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3135509e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30cba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97adbbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = requests.get('https://www.myntra.com/clothing').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec524ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(texts,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d454ff9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
